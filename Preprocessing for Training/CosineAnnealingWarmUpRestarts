{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMnF4DiMvUIB+kkUuwLNDx2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Iq_iMmc-8MC7"},"outputs":[],"source":["import math\n","from torch.optim.lr_scheduler import _LRScheduler\n","\n","class CosineAnnealingWarmUpRestarts(_LRScheduler):\n","    \"\"\"\n","    Scheduler that implements warm-up + Cosine Annealing + restarts.\n","    \"\"\"\n","    def __init__(\n","        self,\n","        optimizer,\n","        T_0,\n","        T_mult,\n","        eta_max,\n","        T_up,\n","        gamma,\n","        last_epoch=-1\n","    ):\n","        if T_0 <= 0 or not isinstance(T_0, int):\n","            raise ValueError(f\"Expected positive integer T_0, but got {T_0}\")\n","        if T_mult < 1 or not isinstance(T_mult, int):\n","            raise ValueError(f\"Expected integer T_mult >= 1, but got {T_mult}\")\n","        if T_up < 0 or not isinstance(T_up, int):\n","            raise ValueError(f\"Expected positive integer T_up, but got {T_up}\")\n","\n","        self.T_0 = T_0\n","        self.T_mult = T_mult\n","        self.base_eta_max = eta_max\n","        self.eta_max = eta_max\n","        self.T_up = T_up\n","        self.T_i = T_0\n","        self.gamma = gamma\n","        self.cycle = 0\n","        self.T_cur = last_epoch\n","        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n","\n","    def get_lr(self):\n","        if self.T_cur == -1:\n","            return self.base_lrs\n","        elif self.T_cur < self.T_up:\n","            return [\n","                (self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr\n","                for base_lr in self.base_lrs\n","            ]\n","        else:\n","            return [\n","                base_lr + (self.eta_max - base_lr) *\n","                (1 + math.cos(math.pi*(self.T_cur - self.T_up) / (self.T_i - self.T_up))) / 2\n","                for base_lr in self.base_lrs\n","            ]\n","\n","    def step(self, epoch=None):\n","        if epoch is None:\n","            epoch = self.last_epoch + 1\n","            self.T_cur = self.T_cur + 1\n","            if self.T_cur >= self.T_i:\n","                self.cycle += 1\n","                self.T_cur = self.T_cur - self.T_i\n","                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n","        else:\n","            if epoch >= self.T_0:\n","                if self.T_mult == 1:\n","                    self.T_cur = epoch % self.T_0\n","                    self.cycle = epoch // self.T_0\n","                else:\n","                    n = int(math.log((epoch / self.T_0*(self.T_mult-1) + 1), self.T_mult))\n","                    self.cycle = n\n","                    self.T_cur = epoch - self.T_0*(self.T_mult**n - 1)/(self.T_mult-1)\n","                    self.T_i = self.T_0 * self.T_mult**n\n","            else:\n","                self.T_i = self.T_0\n","                self.T_cur = epoch\n","\n","        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n","        self.last_epoch = math.floor(epoch)\n","        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n","            param_group['lr'] = lr\n"]}]}
